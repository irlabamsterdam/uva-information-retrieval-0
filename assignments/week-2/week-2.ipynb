{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851387fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before you turn this assignment in, make sure everything runs as expected by going to the menubar and running: \n",
    "\n",
    "**Kernel $\\rightarrow$ Restart & Run All**\n",
    "\n",
    "Please replace all spots marked with `# ADD YOUR CODE HERE` or `ADD YOUR ANSWER HERE`.\n",
    "\n",
    "And start by filling in your name and student_id below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e54372d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "STUDENT_ID = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d95c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert len(NAME) > 0, \"Please fill in your name\"\n",
    "assert len(STUDENT_ID) > 0, \"Please fill in your student id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36098c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61b4e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3724e6928e4c7ebb10746cd0ba9d1228",
     "grade": false,
     "grade_id": "cell-186e1e44f0bd2d60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fa9ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cefaf8e259692af5b8cb9dbe909d610",
     "grade": false,
     "grade_id": "cell-4275800b83eca0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Enable doctests in Jupyter:\n",
    "    def test(fn: Callable):\n",
    "        doctest.run_docstring_examples(fn, globals(), verbose=True, name=fn.__name__)\n",
    "else:\n",
    "    # Disable doctests in autograding setup\n",
    "    def test(fn: Callable): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2890506",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c8b67828959a8b5a870191c001b1b08",
     "grade": false,
     "grade_id": "cell-b4d821343ae97089",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week 2 - Evaluation\n",
    "\n",
    "Welcome back to the second assignment of the search engines course. We will use this assignment to deepen our understanding of the evaluation metrics introduced in this week's lecture. In part I, we look into relevance in information retrieval. In part II, we learn about set-based retrieval metrics. And finally, part III looks at metrics that take the order of our search results into account.\n",
    "\n",
    "For any questions, problems, or feedback, please contact your TA. Good luck with the assignment!\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "üìö [Croft, Metzler, and Strohman - Chapter 8](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf#page=321)\n",
    "\n",
    "üìö [Manning, Raghavan, Sch√ºtze - Chapter 8](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf)\n",
    "\n",
    "üåê [Numpy - Basics](https://numpy.org/doc/stable/user/absolute_beginners.html#what-is-an-array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24459ebb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e60485c44d91e2475adf1e4c83f2f598",
     "grade": false,
     "grade_id": "cell-a87af4ec24b132f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part I - Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41ee9919019911c0b8673e24699d62e8",
     "grade": false,
     "grade_id": "cell-0cb470def48d4267",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 The notion of relevance\n",
    "\n",
    "Last week, we learned about the user's information need and the concept of relevance. So let's begin this assignment on evaluating search engines by refreshing what relevance means.\n",
    "\n",
    "\n",
    "üìù Which of these following statements are correct?\n",
    "\n",
    "a.) A document must contain all query words to be relevant.\n",
    "\n",
    "b.) Relevance can be binary (not relevant / relevant) or graded (e.g., not relevant / somewhat relevant / very relevant).\n",
    "\n",
    "c.) Relevance is assessed over all documents in a corpus.\n",
    "\n",
    "d.) Relevance tries to capture how well a document fits a given query, not a user‚Äôs information need.\n",
    "\n",
    "e.) A common assumption is that the relevance of one document is independent of other documents.\n",
    "\n",
    "f.) Typically, trained search engine evaluators are asked to assess the relevance of search results.\n",
    "\n",
    "g.) Relevance labels are not always valid over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97c21b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d017de3d271fc3d85bf7428df7e7d9f3",
     "grade": false,
     "grade_id": "cell-d70f3cf74a911094",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ADD YOUR ANSWER HERE\n",
    "answer_1_1_a = False\n",
    "answer_1_1_b = False\n",
    "answer_1_1_c = False\n",
    "answer_1_1_d = False\n",
    "answer_1_1_e = False\n",
    "answer_1_1_f = False\n",
    "answer_1_1_g = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b030b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e6f0a1ad9a8407e2852667ef9fc0f6e",
     "grade": false,
     "grade_id": "cell-b666d2d3a2c3fde7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Critique of relevance\n",
    "\n",
    "Document relevance was (and is) arguably a very important concept in information retrieval. However, the concept is not without criticism (üìö [Manning et al.](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf)).\n",
    "\n",
    "üìù List at least three potential issues of the relevance concept and explain each with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62c80a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21da8956b556fb0db1b27c68e07588da",
     "grade": true,
     "grade_id": "test_1_2-critique-relevance",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546af42a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "003bfdfd0347a675838adc6c61e66237",
     "grade": false,
     "grade_id": "cell-43d38ed8270ae9d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 User satisfaction\n",
    "Search engines have to consider many more aspects beyond document relevance to satisfy users. \n",
    "\n",
    "üìù Think of at least three factors beyond document relevance that are expected to impact user satisfaction in web search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113fe5c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2135fb52d3d6b0eca78eaa7693513da2",
     "grade": true,
     "grade_id": "cell-13f1a35bc55d0349",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e994b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b9d399c0ec7566e079ca823035d979b",
     "grade": false,
     "grade_id": "cell-6a19799a138e7fae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part II - Unranked / Set-based Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5100a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b67b9f3e23af5418f2f8381d4f0b51b",
     "grade": false,
     "grade_id": "cell-00fa7bf55cfc234c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1 Precision and recall\n",
    "\n",
    "The most basic evaluation metrics for search engines are set-based. These metrics treat search results as an unordered set of documents and measure how many relevant or non-relevant items were retrieved, but not in which order. The two most common metrics are [precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision) and [recall](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Recall).\n",
    "\n",
    "Precision is the fraction of documents in our search results that are relevant. Looking at the top 20 search result of our Google page, for example, precision tells you how many items of these 20 are actually relevant:\n",
    "\n",
    "$\n",
    "\\textrm{precision} = \\normalsize\\frac{\\textrm{\\# retrieved relevant items}}{\\textrm{\\# retrieved items}}\n",
    "$\n",
    "\n",
    "Recall is the fraction of relevant documents that were retrieved. Meaning, if there are 50 relevant documents overall for our given information need, recall tells you how many of these 50 were retrieved in our search results.\n",
    "\n",
    "$\n",
    "\\textrm{recall} = \\normalsize\\frac{\\textrm{\\# retrieved relevant items}}{\\textrm{\\# all relevant items}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41004042",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea048ddf66804dace39f7884fa4154d",
     "grade": false,
     "grade_id": "cell-7875df2c2bdd1e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ranges of precision and recall\n",
    "üìù Given a collection of 100 documents, of which 60 are relevant. What is the minimum and maximum possible precision and recall if you return 50 documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61918d87",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff1131ce2e8c7fbbf4c385b115ee28f",
     "grade": false,
     "grade_id": "cell-87c6759b3657e01a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ADD YOUR ANSWER HERE\n",
    "min_precision = 0.0\n",
    "max_precision = 0.0\n",
    "\n",
    "min_recall = 0.0\n",
    "max_recall = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44cf1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8db1bc50a61f6ed412a097d05555b051",
     "grade": true,
     "grade_id": "cell-376a5613da1f777e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0 <= min_precision and min_precision <= 1, \"Precision be between 0.0 and 1.0\"\n",
    "assert 0 <= max_precision and max_precision <= 1, \"Precision be between 0.0 and 1.0\"\n",
    "assert 0 <= min_recall and min_recall <= 1, \"Precision be between 0.0 and 1.0\"\n",
    "assert 0 <= max_recall and max_recall <= 1, \"Precision be between 0.0 and 1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79080f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35b0cbe4dcb5f5fe7d89a98efa88aaea",
     "grade": false,
     "grade_id": "cell-5c2ac5574664cec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Precision@k\n",
    "\n",
    "Now, let's actually implement precision and recall. Complete the two functions below. Assume the `scores` parameter is an array of binary document relevance for a given ranking.\n",
    "\n",
    "üìù Begin by implementing [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), which only considers the top k documents for calculating precision.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Note: To make mathematical operations on the scores array easier, the supplied parameter is a numpy array NOT a Python list. You can find an introduction to numpy in the resource section at the top of the notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85239bad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56094b418d22c0362869b967b40ea3bc",
     "grade": false,
     "grade_id": "cell-454f944270c07ef8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def precision(scores: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    >>> precision(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 4)\n",
    "    0.25\n",
    "    >>> precision(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 1)\n",
    "    1.0\n",
    "    >>> precision(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 10)\n",
    "    0.5\n",
    "    >>> precision(np.array([1, 1, 1, 0, 0]), 3)\n",
    "    1.0\n",
    "    >>> precision(np.array([0, 0, 0, 0, 0]), 3)\n",
    "    0.0\n",
    "    >>> precision(np.array([1, 0, 0, 0, 0]), 5)\n",
    "    0.2\n",
    "    >>> precision(np.array([1, 0, 0, 0, 0]), 4)\n",
    "    0.25\n",
    "    >>> precision(np.array([1, 0, 0, 0, 0]), 100)\n",
    "    0.2\n",
    "    \"\"\"\n",
    "    precision = 0.0\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210b509",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "564a414ffb8d13582883603d61695f02",
     "grade": true,
     "grade_id": "test_2_2-impl-precision",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a733a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a5b954e73cb9d1cbda6c6b99159bd19",
     "grade": false,
     "grade_id": "cell-cfd436e30ca4c6e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 Recall@k\n",
    "\n",
    "üìù Next implement recall and similarly consider only the top k documents as your search result. Use the number of all relevant documents in the scores array (even if they are not in the top k) as the total number of relevant documents in the corpus. If there are no relevant documents in the corpus to be retrieved, your recall should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f166f41",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4ada68e37537e01ddb798dd1e610b8c",
     "grade": false,
     "grade_id": "cell-d6785e7f16412762",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def recall(scores: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    >>> recall(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 5)\n",
    "    0.4\n",
    "    >>> recall(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 1)\n",
    "    0.2\n",
    "    >>> recall(np.array([1, 0, 0, 0, 1, 0, 0, 1, 1, 1]), 10)\n",
    "    1.0\n",
    "    >>> recall(np.array([1, 1, 1, 0, 0]), 3)\n",
    "    1.0\n",
    "    >>> recall(np.array([1, 1, 1, 0, 1]), 3)\n",
    "    0.75\n",
    "    >>> recall(np.array([0, 0, 0, 0, 1]), 5)\n",
    "    1.0\n",
    "    >>> recall(np.array([0, 0, 0, 0, 0]), 3)\n",
    "    0.0\n",
    "    >>> recall(np.array([0, 0, 0, 0, 1]), 100)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    recall = 0.0\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea015662",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "864ec404f6cedadef1038a305c951297",
     "grade": true,
     "grade_id": "test_2_3-impl-recall",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e1de2-255f-48fa-8243-f1603bbbeec9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1ed3a2c61a1785a5e527ae5d6d784d9",
     "grade": false,
     "grade_id": "cell-d2d9f308e9a04951",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.4 Precision-Recall Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b99fc-3b3d-44f2-90f5-e1092947b4a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6306d817e13fdeba35adb840aac7690",
     "grade": false,
     "grade_id": "cell-d8cf462ee3621b46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(precision, recall, name=\"Precision-Recall\", stepped=False):\n",
    "    assert len(recall) == len(precision), \"Recall and precision arrays must have the same length\"\n",
    "    df = pd.DataFrame({\"precision\": precision, \"recall\": recall, \"name\": name})\n",
    "\n",
    "    return alt.Chart(df, width=300, height=300).mark_line(\n",
    "        point=True,\n",
    "        interpolate=\"step-before\" if stepped else \"linear\"\n",
    "    ).encode(\n",
    "        x=alt.X(\"recall:Q\", title=\"Recall\").scale(domain=(0, 1)),\n",
    "        y=alt.Y(\"precision:Q\", title=\"Precision\").scale(domain=(0, 1)),\n",
    "        color=alt.Color(\"name\", title=\"\"),\n",
    "        tooltip=[\"precision\", \"recall\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca54171-05db-4c8b-9979-c80f2ae8fce8",
   "metadata": {},
   "source": [
    "### Reading the Precision-Recall Graph\n",
    "\n",
    "Computing precision and recall at different top k ranks is a common way to gain insight into the performance of different ranking algorithms. However, we also loose quite a bit of detail in the process, especially when comparing precision/recall across different queries or entire systems. Precision-recall graphs are one method we've covered in the lecture that can be helpful to get a more detailed view of ranking performance across different recall levels.\n",
    "\n",
    "We show a graph below for the following ranking of relevant/non-relevant items: \n",
    "```\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n",
    "```\n",
    "You read these plots from left to right. We start with a relevant document leading to a precision of 1, followed by a non-relevant document which drops precision to 0.5 but does not change recall. The third document is relevant again leading to a bump in precision and recall, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e514f-bc3a-49fe-9bc0-9def30170805",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "951d371b11b1834c321fa74ccbbce2a4",
     "grade": false,
     "grade_id": "cell-f2938a90dee701b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precision_at_k = np.array([1/1, 1/2, 2/3, 3/4, 3/5, 4/6, 4/7, 4/8, 4/9, 5/10])\n",
    "recall_at_k = np.array([1/5, 1/5, 2/5, 3/5, 3/5, 4/5, 4/5, 4/5, 4/5, 5/5])\n",
    "\n",
    "plot(\n",
    "    precision=precision_at_k,\n",
    "    recall=recall_at_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedb5cd-6189-44ae-b748-33baad941ada",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "862a65c7ef4a644d28088bfdec32c31e",
     "grade": false,
     "grade_id": "cell-e94745c1e8a6f508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Interpolating the Precision-Recall Graph\n",
    "\n",
    "The graph above shows precision and recall for a single query. To average performance across queries, it‚Äôs common to calculate precision at standard recall levels (e.g., 0.0, 0.1, 0.2, etc.). However, since queries have different numbers of relevant documents, their recall levels vary (e.g., a query with three documents can have recall levels 0, 1/3, 2/3, 1.0, while a query with five documents has 0, 0.2, 0.4, 0.6, 0.8, 1.0). To address this, we need a method to calculate precision at fixed recall levels, with the following interpolation method being the most common approach in information retrieval.\n",
    "\n",
    "#### Interpolated precision  (üìö [Croft et al.](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf#page=340)):\n",
    "Given a set $ S $ of observed pairs of precision@k and recall@k values $(P, R)$, the **interpolated precision** at any recall level $R$ is defined as:\n",
    "\n",
    "$\n",
    "\\text{interpolated\\_precision@recall}(R) = \\max \\left\\{P': R' \\geq R \\text{ and } (R', P') \\in S\\right\\}\n",
    "$\n",
    "\n",
    "This means that the interpolated precision at a given recall level $R$ is the highest precision observed at any recall level $R'$ that is greater than or equal to $R$.\n",
    "\n",
    "\n",
    "#### Example\n",
    "Consider the following precision and recall values for different k:\n",
    "\n",
    "```python\n",
    "precision = [0.0, 0.5, 2/3]\n",
    "recall = [0.0, 0.5, 1.0]\n",
    "```\n",
    "\n",
    "To compute the interpolated precision at recall level `0.25`, we first identify relevant recall levels: Look at recall levels greater than or equal to `0.25`. In this case, the relevant recall levels are `0.5` and `1.0`. Second, we find the maximum precision: Among these relevant recall levels, find the maximum precision value. Here, the precision values are `0.5` and `2/3`. Thus, the interpolated precision at recall level `0.25` is `2/3`.\n",
    "\n",
    "\n",
    "#### Implement interpolation for the PR Graph\n",
    "üìù Complete the method below that takes a list of recall levels and returns the interpolated precision values for each recall level by finding the maximum precision at or above each specified recall level. You can visualize your interpolation method with the code below the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21329232-ded0-4278-9588-80d42fa88459",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ff022d196a8c1c0e1c817d9e5ebb305",
     "grade": false,
     "grade_id": "cell-6697014e5dee02ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def interpolate(precision, recall, recall_levels):\n",
    "    \"\"\"\n",
    "    >>> interpolate(precision=np.array([0/1, 1/2, 2/3]), recall=np.array([0/2, 1/2, 2/2]), recall_levels=np.array([0.25])).round(3)\n",
    "    array([0.667])\n",
    "    >>> interpolate(precision=np.array([1, 0.5, 1/3, 0.5, 0.6]), recall=np.array([1/3, 1/3, 1/3, 2/3, 1]), recall_levels=np.array([0, 0.25, 0.5, 0.75, 1.0]))\n",
    "    array([1. , 1. , 0.6, 0.6, 0.6])\n",
    "    >>> interpolate(precision=np.array([0, 0, 1/3, 0.5]), recall=np.array([0/2, 0/2, 1/2, 2/2]), recall_levels=np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]))\n",
    "    array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "    >>> interpolate(precision=np.array([1, 1/2, 1/3, 1/4]), recall=np.array([1/1, 1/1, 1/1, 1/1]), recall_levels=np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]))\n",
    "    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "    >>> interpolate(precision=np.array([1, 1/2, 1/3, 2/4]), recall=np.array([1/2, 1/2, 1/2, 2/2]), recall_levels=np.array([0, 0.25, 0.5, 0.75, 1.0]))\n",
    "    array([1. , 1. , 1. , 0.5, 0.5])\n",
    "    \"\"\"\n",
    "    interpolated_precision = []\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return np.array(interpolated_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9607bcc-0e0c-4fe4-bb55-8ef554bbfdda",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ea777f34ab9a349abca60e71f85dbef",
     "grade": false,
     "grade_id": "cell-f7326cd164009a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8f0bb-4190-4d8a-a8a8-21d65907672b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e1a7d552726826ad2442ea812e82711",
     "grade": false,
     "grade_id": "cell-6746d48ac9158872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "recall_levels = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "interpolated_precision = interpolate(precision_at_k, recall_at_k, recall_levels)\n",
    "\n",
    "(\n",
    "    plot(precision=precision_at_k, recall=recall_at_k, name=\"Precision-Recall\") +\n",
    "    plot(precision=interpolated_precision, recall=recall_levels, name=\"Precision-Recall (Interpolated)\", stepped=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273a37f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bff18905a9f4d3d804eeabf90ba157c0",
     "grade": false,
     "grade_id": "cell-61fb4162447d90bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.5 F1 - Summarizing Precision and Recall\n",
    "\n",
    "To conclude the section on set-based retrieval, we look at a common way to summarize precision and recall into a single score: the [F1 score](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#F-score_/_F-measure). The F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "$\\textrm{harmonic}(R, P) = \\frac{2\\cdot P \\cdot R}{P + R}$\n",
    "\n",
    "$\\textrm{arithmetic}(R, P) = \\frac{P + R}{2}$\n",
    "\n",
    "üìù What is the benefit of using the harmonic mean of precision and recall over the arithmetic mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589eef4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cae351abc0d1b97bf199dfcba9d0b74",
     "grade": true,
     "grade_id": "test_2_4_2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fc127",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c51ef598265523b2a0aa4131237a34e",
     "grade": false,
     "grade_id": "cell-873d0129a52f44d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part III - Ranked Retrieval\n",
    "\n",
    "All metrics so far have ignored the order of search results. Precision does not change if the relevant documents are at the very top or at the very bottom of our search results (as long as they were retrieved). We know, however, that order is important since users tend to look at documents at higher positions more often and usually don't scroll down to the bottom (a phenomena called position bias).\n",
    "\n",
    "After this course, you should be familiar with **three common rank-sensitive metrics: Average Precision (AP), MRR, and nDCG.** In the following, we will implement MRR and nDCG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3cedd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc04a84a44271cececdaca718cf006b5",
     "grade": false,
     "grade_id": "cell-0d7336553700e636",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 Reciprocal Rank\n",
    "\n",
    "A simple, but common, metric in web search is to measure the **rank of the first relevant item** in your search results. Taking the inverse (or reciprocal) of this rank gives you the reciprocal rank measure for a given ranking:\n",
    "\n",
    "$\\text{reciprocal\\_rank} = \\frac{1}{\\text{rank of first relevant item}}$\n",
    "\n",
    "As above, ranks are counted starting at one. If the first search result is relevant your reciprocal rank is 1/1, if the third result is the first relevant document, the reciprocal rank is 1/3. Averaging this metric over multiple rankings gives you the [mean reciprocal rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) for a given search engine.\n",
    "\n",
    "üìù Complete the reciprocal rank function below. Consider only the top k elements of the scores array. Return zero if no document in the ranking is relevant.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Tip: The numpy function \"np.nonzero(array)\" returns the indices of all array entries that are not zero and could be helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8694757",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71b16faac69177fa1db9aa783dd9c04a",
     "grade": false,
     "grade_id": "cell-a48a847cbe451c18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reciprocal_rank(scores: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    >>> reciprocal_rank(np.array([1, 0, 1, 0, 1, 0, 0, 1, 1, 1]), 5)\n",
    "    1.0\n",
    "    >>> reciprocal_rank(np.array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1]), 10)\n",
    "    0.25\n",
    "    >>> reciprocal_rank(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]), 10)\n",
    "    0.1\n",
    "    >>> reciprocal_rank(np.array([0, 0, 0, 0, 1]), 5)\n",
    "    0.2\n",
    "    >>> reciprocal_rank(np.array([0, 0, 0, 0, 1]), 4)\n",
    "    0.0\n",
    "    >>> reciprocal_rank(np.array([0, 0, 0, 0, 1]), 100)\n",
    "    0.2\n",
    "    \"\"\"\n",
    "    reciprocal_rank = 0.0\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319eb36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f640f34f706d8299c39ef9b6674f0a9",
     "grade": true,
     "grade_id": "cell-6ae208ddd8d45f65",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test(reciprocal_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efb660",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29221d88fe14393c6d672486aa62d4ed",
     "grade": false,
     "grade_id": "cell-683a444e2f37eab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Graded relevance and DCG\n",
    "\n",
    "The last metric we will consider in this assignment is the normalized discounted cumulative gain or [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG) metric. This metric makes two assumptions:\n",
    "\n",
    "* First, highly relevant items are more useful than somewhat relevant items. This assumption moves beyond the notion of binary relevance to a graded relevance scale (often on a 5 point scale, from 0 to 4). Graded relevance here signifies the **gain** a user has from looking at a given document.\n",
    "\n",
    "* Second, ranking relevant items lower makes them less useful since the user is less likely to look at them (position bias). Meaning, the relevance of an item is reduced or **discounted** based on its position. \n",
    "\n",
    "Putting those two assumptions together results in the discounted cumulative gain metric (DCG). Let $i$ be the rank of a document in a ranking. Then, the DCG is the relevance of the document discounted by the log of the rank (adding 1 to avoid division by zero) at which it is displayed:\n",
    "\n",
    "$\\textrm{DCG} = \\normalsize \\sum_{i=1}^{N} \\frac{relevance_i}{\\log_2(i + 1)}$\n",
    "\n",
    "üìù Compute the DCG for a given ranking. Note that the score array has now graded relevance labels from 0 to 4. As before take the parameter k into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089071ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cde83b5bd0319b0d33623af665c7b3c8",
     "grade": false,
     "grade_id": "cell-cdf8f21bb915de79",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dcg(scores: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    >>> dcg(np.array([4, 0, 2, 0, 0, 0, 3, 0]), 8)\n",
    "    6.0\n",
    "    >>> dcg(np.array([0, 0, 4, 0, 0, 0, 3, 0]), 8)\n",
    "    3.0\n",
    "    >>> dcg(np.array([0, 0, 1, 0, 0, 0, 3, 0]), 8)\n",
    "    1.5\n",
    "    >>> dcg(np.array([0, 0, 1, 0, 0, 0, 3, 0]), 4)\n",
    "    0.5\n",
    "    >>> dcg(np.array([0, 0, 0, 0, 0, 0, 3, 0]), 4)\n",
    "    0.0\n",
    "    >>> dcg(np.array([0, 0, 1, 0, 0, 0, 3, 0]), 100)\n",
    "    1.5\n",
    "    >>> round(dcg(np.array([4, 3, 2, 1, 0, 0, 0, 0]), 8), 6)\n",
    "    7.323466\n",
    "    >>> round(dcg(np.array([4, 3, 2, 1, 0, 0, 0, 0]), 4), 6)\n",
    "    7.323466\n",
    "    >>> round(dcg(np.array([0, 1, 2, 3, 4]), 5), 6)\n",
    "    4.470371\n",
    "    >>> round(dcg(np.array([0, 1, 2, 3, 4]), 4), 6)\n",
    "    2.922959\n",
    "    >>> round(dcg(np.array([0, 1, 2, 3, 4]), 3), 6)\n",
    "    1.63093\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bdc3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "316caeaec6344d8ae58df63339258a41",
     "grade": true,
     "grade_id": "cell-b522327116f1b8bf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test(dcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c55632",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75a81c4ab0ac9fc38296e7fc81792591",
     "grade": false,
     "grade_id": "cell-6e6d92c46c1d8d48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 nDCG\n",
    "\n",
    "Since the value of DCG depends on the relevance of the documents in the ranking (and thus varies highly between different rankings / queries), it is usually normalized by the maximum DCG possible for a given ranking. I.e., we normalize by the score for the ideal ranking of the documents in a given ranking:\n",
    "\n",
    "$\\textrm{nDCG} = \\normalsize \\frac{\\textrm{DCG}}{\\textrm{Ideal DCG}}$\n",
    "\n",
    "Consider this example ranking of five documents with a relevance between 0 and 4.\n",
    "```\n",
    "[2, 3, 1, 4, 0],  DCG ‚âà 6.12\n",
    "```\n",
    "\n",
    "While the ideal ranking would result in a DCG of:\n",
    "```\n",
    "[4, 3, 2, 1, 0], DCG ‚âà 7.32\n",
    "```\n",
    "\n",
    "Dividing the DCG of our ranking by its ideal version results in an nDCG of 0.84. Note that through this normalization nDCG is always between 0 and 1.\n",
    "\n",
    "\n",
    "üìù Reuse the above dcg function to compute the nDCG score for a ranking. Take the cutoff parameter k into account. If there is no relevant document in your ranking, and the ideal DCG is 0, return 0 as the nDCG value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91167627",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "757c85c20652125ca3ff0c7a586c12cc",
     "grade": false,
     "grade_id": "cell-98cc8f958c79c07d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ndcg(scores: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    >>> ndcg(np.array([4, 3, 2, 1, 1, 1, 0, 0]), 8)\n",
    "    1.0\n",
    "    >>> round(ndcg(np.array([0, 0, 1, 1, 1, 2, 3, 4]), 8), 6)\n",
    "    0.532051\n",
    "    >>> round(ndcg(np.array([4, 0, 1, 1, 1, 2, 3, 0]), 8), 6)\n",
    "    0.871496\n",
    "    >>> ndcg(np.array([0, 0, 0, 0, 4, 3, 2, 1]), 4)\n",
    "    0.0\n",
    "    >>> round(ndcg(np.array([4, 0, 1, 1, 1, 2, 3, 0]), 100), 6)\n",
    "    0.871496\n",
    "    >>> ndcg(np.array([0, 0, 0, 0]), 3)\n",
    "    0.0\n",
    "    \"\"\"\n",
    "    ndcg = 0.0\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de9b62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8eec340fbbf022215640c91f2a2a915b",
     "grade": true,
     "grade_id": "cell-024a28f9e282f775",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test(ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86db9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d476df612f0f7210ade2de7d3e1eec4d",
     "grade": false,
     "grade_id": "cell-9e1f4460368c66b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.4 Contrasting nDCG and MRR\n",
    "\n",
    "MRR and nDCG measure different aspects of a search engine, and they do not not always have to agree with each other.\n",
    "\n",
    "üìù Construct two rankings A and B below so that:\n",
    "\n",
    "- **A** has a higher MRR but a lower nDCG than B\n",
    "- **B** has a lower MRR but a higher nDCG than A \n",
    "\n",
    "Use only exactly five documents with relevance: [0, 1, 2, 3, 4]\n",
    "\n",
    "Since reciprocal rank considers only binary relevance, consider the single document with relevance 4 as the only relevant document in a binary setting. We added some boilerplate code to convert your answer to binary relevance to use your reciprocal_rank function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4dcd3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cad0050efd02ea89f4b377095048da8c",
     "grade": false,
     "grade_id": "cell-3833660d5e48efe7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ranking_a = np.array([\n",
    "    # ADD YOUR CODE HERE\n",
    "])\n",
    "ranking_b = np.array([\n",
    "    # ADD YOUR CODE HERE\n",
    "])\n",
    "\n",
    "# Convert to binary relevance:\n",
    "binary_ranking_a = np.where(ranking_a == 4, 1, 0)\n",
    "binary_ranking_b = np.where(ranking_b == 4, 1, 0)\n",
    "\n",
    "print(f\"Ranking A, nDCG: {ndcg(ranking_a, 5)}, MRR: {reciprocal_rank(binary_ranking_a, 5)}\")\n",
    "print(f\"Ranking B, nDCG: {ndcg(ranking_b, 5)}, MRR: {reciprocal_rank(binary_ranking_b, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9badd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55fa3214cfe86678ad81047cc2b8b049",
     "grade": false,
     "grade_id": "cell-8c631a89ed5eb335",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.5 When to optimize for nDCG or MRR?\n",
    "\n",
    "üìù Lastly, in which real-world search settings would ranking A (high MRR) or B (high nDCG) be preferrable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac98a9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d554f5341db77eb6a816c1f9225facc4",
     "grade": true,
     "grade_id": "cell-0ea559cd13a62660",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
