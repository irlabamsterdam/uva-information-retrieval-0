{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535c6b9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before you turn this assignment in, make sure everything runs as expected by going to the menubar and running: \n",
    "\n",
    "**Kernel $\\rightarrow$ Restart & Run All**\n",
    "\n",
    "Please replace all spots marked with `# ADD YOUR CODE HERE` or `ADD YOUR ANSWER HERE`.\n",
    "\n",
    "And start by filling in your name and student_id below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a4594",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "STUDENT_ID = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8379346",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert len(NAME) > 0, \"Please fill in your name\"\n",
    "assert len(STUDENT_ID) > 0, \"Please fill in your student id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729254c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec072e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17b2004b3d016ea0a6b14aae7c6133a6",
     "grade": false,
     "grade_id": "cell-65044aea77bd9297",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c695d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7102e983268673430d0f29dac0ae4692",
     "grade": false,
     "grade_id": "cell-b0ee77ce46a2e097",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test(fn: Callable):\n",
    "    # Turn off doctests in autograding:\n",
    "    if __name__ == \"__main__\":\n",
    "        doctest.run_docstring_examples(fn, globals(), verbose=True, name=fn.__name__, optionflags=doctest.ELLIPSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7148f2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c81b7cc2d5c68daf6d25572a12ea542",
     "grade": false,
     "grade_id": "cell-cc16024ba3125e21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week 4 - Learning to Rank & Vocabulary Mismatch\n",
    "\n",
    "Welcome to week four of Zoekmachines! üëã\n",
    "\n",
    "In part I of this week's assignment, we will learn how to automatically combine features for ranking. These methods are more computationally expensive but also more powerful than the ranking approaches we discussed in previous weeks. It is common to use a fast and simple method to retrieve a candidate set of items (e.g., retrieve a top 1000 list using bm25) and then apply a slower but more advanced learning-to-rank model to create the final ranking.\n",
    "\n",
    "In part II, we re-visit the problem of vocabulary mismatch and how Latent Semantic Indexing might help to address the issue.\n",
    "\n",
    "Compared to previous weeks, this week is more library-heavy since you will learn some tools that are actually used in real-world industry applications.\n",
    "\n",
    "Good luck with the assignment!\n",
    "\n",
    "### Resources\n",
    "üìö [Latent Semantic Indexing - Manning, Raghavan, Sch√ºtze - Chapter 18.4](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf)\n",
    "\n",
    "üåê [Scikit-Learn TfidfVectorizer API](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "üåê [Scikit-Learn LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "üåê [Learning to rank with XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c1d1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db370c600c3f8256292e6647ab0d5873",
     "grade": false,
     "grade_id": "cell-381d1d86c336aa0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part I - Learning to rank (LTR)\n",
    "\n",
    "In previous weeks, we've looked at basic ranking methods such as tf-idf, bm25, or language models. However, the impact of different features (such as tf, idf, or document length) on the overall ranking is tuned manually and not learned automatically, making it difficult to introduce additional features. Modern search engines consider hundreds of features for their ranking. So this week, we will automatically learn how to weigh different features to create better rankings.\n",
    "\n",
    "We will investigate two approaches: (1) a pointwise approach using linear regression and (2) a pairwise approach employing gradient-boosted decision trees.\n",
    "\n",
    "Let's begin by loading a larger version of the [covid TREC dataset](https://ir.nist.gov/trec-covid/) from last week. The dataset contains pairs of real search queries and their candidate documents (100 docs per query) to be ranked. Each query-document pair is accompanied by a human relevance judgmenet on a scale of 0, 1, 2. We load three datasets: A train dataset, a test dataset for evaluation, and a combination of both, which we use during indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d67d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b15c8a35f690d163aab3aa7b0ef9b07",
     "grade": false,
     "grade_id": "cell-c8bf235a19dfa530",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(url: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"body\"] = df[\"body\"].fillna(\"\")\n",
    "    return df\n",
    "\n",
    "train_df = load_data(\"https://raw.githubusercontent.com/irlabamsterdam/uva-ir0-assignments/main/data/trec-covid-train.csv\")\n",
    "test_df = load_data(\"https://raw.githubusercontent.com/irlabamsterdam/uva-ir0-assignments/main/data/trec-covid-test.csv\")\n",
    "df = pd.concat([train_df, test_df])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b93bc2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f595f0540f3edce88f7c371f6a3455e1",
     "grade": false,
     "grade_id": "cell-c83d6f32777cb444",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Feature engineering: TF-IDF vectors\n",
    "\n",
    "Before learning how to weight features, we need to create our features. As you can imagine there are plenty of potential features that are useful during ranking. Here, for example, is a list of common real-world features used in [Microsoft Bing](https://www.microsoft.com/en-us/research/project/mslr/). We will compute features based on tf-idf weighting introduced last week. But instead of manually implementing an inverted index and repeating all of last week's calculation, we will use a more comfortable method and use scikit-learn's [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for document preprocessing and tf-idf computation.\n",
    "\n",
    "üìù The [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class can perform a.o. tokenization, normalization, stopping, and it can transform a piece of raw text directly into a vector of normalized tf-idf values. In the following, create a TfidfVectorizer that:\n",
    "\n",
    "1. Lowercases all tokens.\n",
    "2. Removes English stopwords.\n",
    "3. Uses a sublinear log scaling for the term frequency like we used last week (`1 + log(tf)` instead of using the raw token count `tf`).\n",
    "\n",
    "Return a vectorizer that is trained on all documents in the `documents` list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c0323",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5e0b4b6a6511405373c6e27b2d3f90b",
     "grade": false,
     "grade_id": "cell-abfda053e612a68b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def fit_vectorizer(df: pd.DataFrame) -> TfidfVectorizer:\n",
    "    \"\"\"\n",
    "    # Check size of indexed vocabulary\n",
    "    >>> vectorizer = fit_vectorizer(df)\n",
    "    >>> len(vectorizer.vocabulary_)\n",
    "    40111\n",
    "    \n",
    "    # Embed a single document \"covid 19 pandemic\" and check positions in vector\n",
    "    >>> vector = vectorizer.transform([\"covid 19 pandemic\"])\n",
    "    >>> np.array_equal(vector.nonzero()[1], np.array([838, 10456, 27452]))\n",
    "    True\n",
    "    \n",
    "    # Embed a single document \"covid 19 pandemic\" and check tfidf values\n",
    "    >>> vector = vectorizer.transform([\"covid 19 pandemic\"])\n",
    "    >>> np.allclose(vector[vector.nonzero()], np.array([[0.47729191, 0.4755531, 0.73894633]]))\n",
    "    True\n",
    "    \"\"\"\n",
    "    vectorizer = None\n",
    "    documents = list(df[\"query\"]) + list(df[\"title\"]) + list(df[\"body\"])\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd265ba7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45f743d6df6cf9a809b3340cc45d2a22",
     "grade": true,
     "grade_id": "cell-300657a18d9c3522",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(fit_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be95d93-17a6-4098-8562-7d3d11ccf6d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f8e0bd7f2741784152d6a053d97ae87",
     "grade": false,
     "grade_id": "cell-97704fbbbfa6ad3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vectorizer = fit_vectorizer(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5d488",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a36c76d96dd46ab3523ee6703282e37b",
     "grade": false,
     "grade_id": "cell-c752adf9432865c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 - Feature Engineering\n",
    "\n",
    "Next, we use the vectorizer to engineer our features. We will only compute four features. Note that real search engines employ hundreds of features ([Yahoo!'s public LTR dataset contains 700 for example](http://proceedings.mlr.press/v14/chapelle11a/chapelle11a.pdf)). These features might depend only on the document (e.g., document length) and can be computed in advance, or they might depend on the current query and have to be computed on the fly.\n",
    "\n",
    "Our train dataset contains 50 search queries each with 100 candidate documents. We already created query-document pairs so that each row of our dataset contains a document with `title` and `body` and a matching search `query`.\n",
    "\n",
    "üìù Use the TfidfVectorizer supplied in the method below to **create vectors for the search query, document title, and body text**. Use these three vectors to compute the following features and return them in a new pandas DataFrame:\n",
    "\n",
    "* `title_tfidf`: The cosine similarity between the query vector and title vector of the document.\n",
    "* `body_tfidf`: The cosine similarity between the query vector and body vector of the document.\n",
    "* `title_overlap`: The number of unique matching terms between query and title divided by the number of unique query terms.\n",
    "* `body_overlap`: The number of unique matching terms between the query and body divided by the number of unique query terms.\n",
    "\n",
    "Unique matches in this context means that you can ignore repeating tokens inside the query and count them as one. E.g., if the query is \"coronavirus\" and the document contains the word twice, its counted as one match. Note that you should compute the overlap features also with the tf-idf vectors and not start to tokenize text into individual words in this task.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Tip: Note that the vectorizer returns sparse vectors which save a lot of memory since they only store nonzero entries. However, some operations might not be supported on them. If necessary, call \".todense()\" on the vectors to transform them into normal numpy representations.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Tip: Scikit-learn's \"sklearn.metrics.pairwise.cosine_similarity\" might be helpful.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Tip: Numpy's \"np.logical_and\" operation might come in handy when finding common entries between two vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bab24",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9488ed7e77d38ed23abf7d00969c7e43",
     "grade": false,
     "grade_id": "cell-6bd08141e9f988a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_features(vectorizer: TfidfVectorizer, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> feature_df = get_features(vectorizer, train_df.head(1))\n",
    "    >>> list(feature_df.columns)\n",
    "    ['query_id', 'relevance', 'title_tfidf', 'body_tfidf', 'title_overlap', 'body_overlap']\n",
    "    \n",
    "    >>> get_features(vectorizer, train_df.head(3)).round(3)\n",
    "       query_id  relevance  title_tfidf  body_tfidf  title_overlap  body_overlap\n",
    "    0         1          1          0.0       0.144            0.0           1.0\n",
    "    1         1          0          0.0       0.010            0.0           0.5\n",
    "    2         1          0          0.0       0.013            0.0           0.5\n",
    "    \n",
    "    >>> get_features(vectorizer, train_df.tail(3)).round(3)\n",
    "       query_id  relevance  title_tfidf  body_tfidf  title_overlap  body_overlap\n",
    "    0        50          2        0.090       0.043          0.333         0.667\n",
    "    1        50          1        0.109       0.102          0.667         0.667\n",
    "    2        50          0        0.000       0.012          0.000         0.333\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        title_tfidf = 0\n",
    "        body_tfidf = 0\n",
    "        title_overlap = 0\n",
    "        body_overlap = 0\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        \n",
    "        rows.append({\n",
    "            \"query_id\": row[\"query_id\"],\n",
    "            \"relevance\": row[\"relevance\"],\n",
    "            \"title_tfidf\": title_tfidf,\n",
    "            \"body_tfidf\": body_tfidf,\n",
    "            \"title_overlap\": title_overlap,\n",
    "            \"body_overlap\": body_overlap,\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f6599",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bc0a5deab69ae1167a2f3a93f8a7915",
     "grade": true,
     "grade_id": "cell-39046887374d0c45",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(get_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a60ff-f766-4de0-be50-e8ab7a4bf88a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f3466578c4f9570c1629dcf65c7eff2",
     "grade": false,
     "grade_id": "cell-48b3abff8e039b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_feature_df = get_features(vectorizer, train_df)\n",
    "    test_feature_df = get_features(vectorizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18217a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74da7b48024d6f7829dbea32a09027c9",
     "grade": false,
     "grade_id": "cell-5ff57f6725ff0fc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's generate features on the train and test datasets using your implementation above. We also add a helper method that evaluates the `nDCG@10` and `nDCG@100` when ranking by each feature on its own.\n",
    "\n",
    "There is no task here, just execute the following cells and inspect the resulting ranking performance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e8b492",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e660dfde6a7a481267eaebfe971bbcb",
     "grade": false,
     "grade_id": "cell-7b0ff785ca41bcdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(df: pd.DataFrame, score_column: str) -> Dict:\n",
    "    df = df.groupby([\"query_id\"]).agg(\n",
    "        y=(\"relevance\", list),\n",
    "        y_predict=(score_column, list)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"score\": score_column,\n",
    "        \"nDCG@10\": ndcg_score(list(df.y), list(df.y_predict), k=10),\n",
    "        \"nDCG@100\": ndcg_score(list(df.y), list(df.y_predict), k=100),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69164f7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc64e128a63da1182c2e9c8cfbf55b33",
     "grade": false,
     "grade_id": "cell-7a6bb43358e01fba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(evaluate(test_feature_df, \"title_tfidf\"))\n",
    "    print(evaluate(test_feature_df, \"title_overlap\"))\n",
    "    print(evaluate(test_feature_df, \"body_tfidf\"))\n",
    "    print(evaluate(test_feature_df, \"body_overlap\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51846960",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3c5bb90f130a3238d823411b920bdf9",
     "grade": false,
     "grade_id": "cell-7bd6d07e94e5bf57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Pointwise LTR\n",
    "\n",
    "Now, let's build our first LTR model. To recap, in our dataset we have a search query $q$. Each query-document pair is represented using a feature vector containing `['title_tfidf', 'body_tfidf', 'title_overlap', 'body_overlap']`, let's call it $x$. And for each pair, we have a relevance annotation obtained by asking human judges (in our dataset the column `relevance`).\n",
    "\n",
    "The pointwise approach ignores which query-document pairs belong to the same query and just learns a function $f(x)$ that maps from our feature vectors as closely as it can to our relevance labels $y$. A classic approach is to use a linear regression model $f(x) = wx + b$, where the weights $w$ and the bias term $b$ are parameters that we need to learn by minimizing the mean squared error loss over all query-document pairs in our dataset:\n",
    "\n",
    "$\\sum_{i}^m (f(x_i) - y_i) ^ 2$\n",
    "\n",
    "üìù In the following, use scikit-learn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to implement a pointwise ranking approach that predict the relevance score from our document features. Complete the class below:\n",
    "\n",
    "1. Create and train a new regression model inside the `fit` method.\n",
    "2. Use the trained model in the `predict` step to predict the relevance score for a dataframe of unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46717656",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51c8fce7bd00dfdc440b6311b279caa8",
     "grade": false,
     "grade_id": "cell-ae6ba5ffeb0c1b3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def test_ranker():\n",
    "    df = pd.DataFrame({\n",
    "        \"query_id\": [0, 0],\n",
    "        \"title_tfidf\": [0, 1],\n",
    "        \"body_tfidf\": [0.5, 1],\n",
    "        \"title_overlap\": [0, 0.5],\n",
    "        \"body_overlap\": [1, 0],\n",
    "        \"relevance\": [0, 5]\n",
    "    })\n",
    "    ranker = PointwiseRanker()\n",
    "    ranker.fit(df)\n",
    "    return ranker, df\n",
    "\n",
    "\n",
    "class PointwiseRanker:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        # Check the learned feature weights and bias on a toy dataset:\n",
    "        >>> ranker, _ = test_ranker()\n",
    "        >>> np.allclose(ranker.model.coef_, np.array([ 2.,  1.,  1., -2.]))\n",
    "        True\n",
    "        >>> round(ranker.model.intercept_, 4)\n",
    "        1.5\n",
    "        \"\"\"\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        ...\n",
    "    \n",
    "    def predict(self, test_df: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        # Check predictions on a toy dataset:\n",
    "        >>> ranker, df = test_ranker()\n",
    "        >>> np.allclose(ranker.predict(df), df.relevance)\n",
    "        True\n",
    "        \"\"\"\n",
    "        \n",
    "        predicted_relevance = np.array([])\n",
    "        \n",
    "        # ADD YOUR CODE HERE\n",
    "        \n",
    "        return predicted_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d403d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6dd2a42e1b38df3749ee0507bc7689c",
     "grade": true,
     "grade_id": "cell-a5143e8e5de769d3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(PointwiseRanker.fit)\n",
    "test(PointwiseRanker.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f4674",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f30602ab8278b3420f4701da52b5589",
     "grade": false,
     "grade_id": "cell-0983098fd9bc69f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = PointwiseRanker()\n",
    "    model.fit(train_feature_df)\n",
    "    test_feature_df[\"pointwise\"] = model.predict(test_feature_df)\n",
    "    print(evaluate(test_feature_df, \"pointwise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f3315",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22a9e939608f27c81c0e14d9138255ec",
     "grade": false,
     "grade_id": "cell-de678e46a4c6b82e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Pairwise LTR\n",
    "\n",
    "Instead of independently predicting a relevance score for each feature vector, pairwise approaches learn relative preference between two document candidates belonging to the same query. Meaning, we learn if one feature vector $x_i$ is more relevant than another $x_j$ for a given query, i.e. whether $y_i > y_j$.\n",
    "\n",
    "üìù In the following, implement the pairwise ranking method RankNet that we've learned in this week's lecture using the [gradient boosted decision tree library XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html):\n",
    "\n",
    "1. Configure the `XGBRanker` class to use the RankNet objective.\n",
    "2. Configure how the algorithm should construct document pairs. Use the `mean` construction method and sample `1` random pair for each document in our query. Note that to construct document pairs, the algorithm needs information about which documents belong to the same query (`qid`).\n",
    "3. Configure your gradient boosted decision tree, use `10` estimators with a max depth of `5` each.\n",
    "4. Set the random_state to `0` to make your code reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da398b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "212c66609df4fe5a37f6c877f582484a",
     "grade": false,
     "grade_id": "cell-e7d098ccd3045602",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRanker\n",
    "\n",
    "\n",
    "def test_ranker():\n",
    "    df = pd.DataFrame({\n",
    "        \"query_id\": [0, 0, 1, 1],\n",
    "        \"title_tfidf\": [0, 1, 0, 0.5],\n",
    "        \"body_tfidf\": [0.5, 1, 0.5, 1],\n",
    "        \"title_overlap\": [0, 0.5, 0, 1],\n",
    "        \"body_overlap\": [1, 0, 1, 0],\n",
    "        \"relevance\": [0, 1, 0, 1]\n",
    "    })\n",
    "    ranker = PairwiseRanker()\n",
    "    ranker.fit(df)\n",
    "    return ranker, df\n",
    "\n",
    "\n",
    "class PairwiseRanker:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        # Check the pairwise model on a toy dataset:\n",
    "        >>> ranker, df = test_ranker()\n",
    "        >>> ranker.model.get_params()[\"objective\"] # Model uses correct objective\n",
    "        'rank:pairwise'\n",
    "        >>> ranker.model.get_params()[\"random_state\"] # Model uses correct random state\n",
    "        0\n",
    "        >>> ranker.model.get_params()[\"lambdarank_pair_method\"] # Model constructs pairs in the correct way\n",
    "        'mean'\n",
    "        \"\"\"\n",
    "        \n",
    "        # ADD YOUR CODE HERE\n",
    "        ...\n",
    "    \n",
    "    def predict(self, test_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        # Check the pairwise model predictions on a toy dataset:\n",
    "        >>> ranker, df = test_ranker()\n",
    "        >>> ranker.predict(df).round(2)\n",
    "        array([-0.74,  0.74, -0.74,  0.74], dtype=float32)\n",
    "        \"\"\"\n",
    "        predicted_relevance = np.array([])\n",
    "        \n",
    "        # ADD YOUR CODE HERE\n",
    "        \n",
    "        return predicted_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c11f17-5622-48a4-ad0e-65df6f336bc8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1e65dfb90083edfb9e8899cdf8754e7",
     "grade": true,
     "grade_id": "cell-ea7b99a95857b8ea",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(PairwiseRanker.fit)\n",
    "test(PairwiseRanker.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7256776",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75214de7980c2a8d600f263b52cb582",
     "grade": false,
     "grade_id": "cell-1fbbba72b36c2e19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = PairwiseRanker()\n",
    "    model.fit(train_feature_df)\n",
    "    test_feature_df[\"pairwise\"] = model.predict(test_feature_df)\n",
    "    print(evaluate(test_feature_df, \"pairwise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a151f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d2f4305d973cfd3ac84c6fdeef42033",
     "grade": false,
     "grade_id": "cell-2d08747fe4a3a8b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5.1 Motivation listwise loss\n",
    "\n",
    "üìù Now that we've build a simple pointwise and pairwise LTR model, what is a listwise LTR model? What is the motivation to use a listwise loss over a pointwise or a pairwise appraoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a5afa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5b80a01753d1faaeb7f5c60c3c2064e",
     "grade": true,
     "grade_id": "cell-6363886650f62b5c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2e2ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "069bd3996e983f8bd579a10ef7321fe3",
     "grade": false,
     "grade_id": "cell-ec4bef6d3394e551",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5.2 Comparing results\n",
    "\n",
    "Lastly, compare how the pointwise, pairwise, and listwise results compare against each other on our covid dataset. You can try out a listwise LambdaMART approach by simply switching the objective in your pairwise implementation to `\"rank:ndcg\"`.\n",
    "\n",
    "Describe also how the three LTR approaches compare against ranking using our initial features from task 1.2.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "‚ö†Ô∏è Please make sure to switch back to the pairwise objective before submitting your notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2a0ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b49020ccc34e6351f0a2c8a23c9243a",
     "grade": true,
     "grade_id": "cell-6db5acd6e94259d0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabf74e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e983defe02974d568aa37b81ca4a9f1",
     "grade": false,
     "grade_id": "cell-326a920bbf6741ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part II - Latent Semantic Indexing (LSI)\n",
    "\n",
    "The vector space model used in the previous week (and the one above) suffers from the vocabulary mismatch problem (see the assignment in week-1). Two issues are particularly common: \n",
    "\n",
    "- Synonyms: Multiple words refer to the same concept: car, automobile, ...\n",
    "- Polysemy: One word refers to multiple concepts: light (color, not heavy, not serious)\n",
    "\n",
    "Synonyms can lead to underscoring relevant documents if they use a synonym of our search query, and polysemy can lead to retrieving false-positive documents for our search query. Instead of searching for exact matches in our document-term matrix, the idea of LSI is to represent documents by groups of related words (topics) rather than a set of fixed terms.\n",
    "\n",
    "In this part, we take a look at computing topics using Singular Value Decomposition (SVD) on NYTimes news article headlines. For that, first download a new dataset containing news headlines from the NYTimes published between 2015 and 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a87fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2979c4356e2e12340a4eef5974d60e0",
     "grade": false,
     "grade_id": "cell-69776c7dd6d71e89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "article_df = pd.read_csv(\"https://raw.githubusercontent.com/irlabamsterdam/uva-ir0-assignments/main/data/nytimes.csv\")\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdb5e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffd79dc37f59268144dfab6bd9a75459",
     "grade": false,
     "grade_id": "cell-400d51675259682b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1 Approximating the term-document matrix\n",
    "\n",
    "One way to compute topics is to use SVD to factorize our term-document-matrix $A$ into three matrices of lower rank that approximate its original entries:\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/82407SVD.png)\n",
    "\n",
    "üìù Describe the inputs and outputs of LSI:\n",
    "1. Describe what $m$, $n$, and $r$ are in the figure above in the context of LSI (meaning when it comes to terms and documents).\n",
    "2. What information is contained in matrix $U$, $\\Sigma$, and $V^T$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ac4ec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcbc9ee688b6160e45e0b51f7ba525c5",
     "grade": true,
     "grade_id": "cell-d9db5b09b9881ff7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">ADD YOUR ANSWER HERE</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f571468-87a8-4560-8269-8d97aab4c3b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "768386b542da4e9756cbd06989b07ceb",
     "grade": false,
     "grade_id": "cell-1d95bd7332c6119d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Constructing the term-document matrix\n",
    "\n",
    "Let's implement LSI using SVD. We begin by creating a term-document matrix (of shape terms x docs) from our news headlines. Instead of just a binary encoding of terms and documents (like in this week's lecture slides), another common choice is to use tf-idf values in the term-document matrix.\n",
    "\n",
    "\n",
    "üìù  Complete the method below:\n",
    "\n",
    "1.\tCreate a term-document matrix containing tf-idf values. Use the [TfidfVectorizer from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Configure the vectorizer to:\n",
    "\t- Remove English stopwords.\n",
    "\t- Only include terms that occur in at least 5 documents (document frequency).\n",
    "2.\tReturn the term-document matrix (with shape terms x docs) and a list of vocabulary terms created by the vectorizer. This list should be sorted by the token ID assigned by the vectorizer, allowing you to look up words based on their ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b8488-a4c3-4863-976f-e86e508ec782",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98f05b5f52f85f6d1f88509e3a194352",
     "grade": false,
     "grade_id": "cell-2c74ab7e36f4c08d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_term_document_matrix(df: pd.DataFrame) -> np.array:\n",
    "    \"\"\"\n",
    "    >>> term_doc_matrix, vocabulary = get_term_document_matrix(article_df)\n",
    "    >>> term_doc_matrix.shape\n",
    "    (3168, 10732)\n",
    "    >>> term_doc_matrix.data[:10].round(2) # Check first ten entries in sparse term-document matrix\n",
    "    array([0.53, 0.69, 0.46, 0.09, 0.09, 0.09, 0.06, 0.06, 0.06, 0.54])\n",
    "    >>> len(vocabulary)\n",
    "    3168\n",
    "    >>> vocabulary[2928], vocabulary[571], vocabulary[1138], vocabulary[1448]\n",
    "    ('trump', 'college', 'football', 'internet')\n",
    "    >>> vocabulary[-10:]\n",
    "    ['york', 'yorkers', 'young', 'younger', 'youth', 'youtube', 'zero', 'zika', 'zimbabwe', 'zone']\n",
    "    \"\"\"\n",
    "    term_doc_matrix = np.array([])\n",
    "    vocabulary = []\n",
    "\n",
    "    # Treat each news headline as a document:\n",
    "    text = list(df.title)\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    return term_doc_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ebd4e-49b7-428a-b758-d7b8df2a38a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72fa8e6832dd7b88ac7910bc907e274b",
     "grade": true,
     "grade_id": "cell-1e727f50207234d3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(get_term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac0201-9f27-4cd5-bbca-845be6c2475f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1621de68f5481bac93f05dcf0a16293b",
     "grade": false,
     "grade_id": "cell-6e651876786fc5ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    term_doc_matrix, vocabulary = get_term_document_matrix(article_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49ebad-d15c-4955-b021-dd270c781df7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc155f19612791c0209699e41b1285c3",
     "grade": false,
     "grade_id": "cell-42c36aec377daf80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 Using SVD to compute term similarity\n",
    "\n",
    "üìù  Next, we will use SVD to approximate our term-document matrix using three smaller matrices. As we discussed above, each matrix contains different information and is helpful in solving different tasks. We want to use SVD to find semantically similar terms for this assignment. Thus, we only need one of the three matrices, the one of shape `terms x topics` (see the task above or check the lecture slides).\n",
    "\n",
    "\n",
    "Implement the method below and use [scikit-learn's TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) to approximate the term-document matrix. Your matrix should be of the shape `terms x topics.`\n",
    "1. Make the number of topics/components used by SVD a parameter of your method. Use 50 iterations and fix the random state to 42.\n",
    "3. Next, compute the cosine similarity between all terms using their topic vectors. The resulting matrix should be of the shape `terms x terms.`\n",
    "4. Lastly, set the cosine similarity of each term to itself (the diagonal) to `-1` and return the final similarity matrix.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "üí° Tip: Scikit-learn's \"sklearn.metrics.pairwise.cosine_similarity\" might be helpful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c32976-8a51-45b1-a64e-265e284b7346",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7b1ef86fbb60641af68da8b9760ee92",
     "grade": false,
     "grade_id": "cell-4d812779aaf7f87f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_term_similarity_matrix(term_doc_matrix, n_components):\n",
    "    \"\"\"\n",
    "    >>> term_similarity = get_term_similarity_matrix(term_doc_matrix, n_components=10)\n",
    "    >>> term_similarity.shape\n",
    "    (3168, 3168)\n",
    "    >>> (np.diag(term_similarity) == -1).all()\n",
    "    True\n",
    "    >>> np.allclose( # If true, n_components was probably not used.\n",
    "    ...    get_term_similarity_matrix(term_doc_matrix, n_components=5),\n",
    "    ...    get_term_similarity_matrix(term_doc_matrix, n_components=25))\n",
    "    False\n",
    "    >>> np.allclose( # If false, random_state is probably not fixed (should be 42).\n",
    "    ...    get_term_similarity_matrix(term_doc_matrix, n_components=10),\n",
    "    ...    get_term_similarity_matrix(term_doc_matrix, n_components=10))\n",
    "    True\n",
    "    \"\"\"\n",
    "    term_similarity = np.array([])\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    return term_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761953e2-6a43-4dbd-86db-6499ad9fd0de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4864cf624098772df02d37b6c7300b17",
     "grade": true,
     "grade_id": "cell-3ff1ae5b04018f53",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(get_term_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb999bdb-5c7b-4449-a3de-3449af6347d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2954a66aa40384d10d27934075e7cad2",
     "grade": false,
     "grade_id": "cell-c3f181aae614d8f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    term_similarity_matrix = get_term_similarity_matrix(term_doc_matrix, n_components=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc684a51-463d-4e90-9eab-2f8e5b3468ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b5a1b5f2d0d9892ad633ad5cc12b548",
     "grade": false,
     "grade_id": "cell-4d9354cc0cca60c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.4 Finding similar terms\n",
    "\n",
    "üìù  Lastly, let's put everything together and find semantically similar tokens in our collection of news headlines. Complete the method below to find the top-n most similar terms for a given term with our similarity matrix (sorting by cosing similarity in descending order). Use the vocabulary to match between terms and their position in the similarity matrix.\n",
    "\n",
    "In the lecture, we discussed the importance of chosing the number of latent components. If you want to see the impact yourself, play around with the `n_components` of the `get_term_similarity_matrix` method above and inspect how the list below changes. But make sure that you reset the number of components back to 200 before submitting your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae5202-84a2-4848-8120-ca70d07a17ad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7900aed87c2204091bcafd491fe3d6e6",
     "grade": false,
     "grade_id": "cell-a917e358d8c14fff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_similar_terms(term_similarity_matrix, vocabulary, term, top_n: int):\n",
    "    \"\"\"\n",
    "    >>> find_similar_terms(term_similarity_matrix, vocabulary, term=\"joe\", top_n=5)\n",
    "    ['biden', 'run', 'presidential', 'taking', '2016']\n",
    "    >>> find_similar_terms(term_similarity_matrix, vocabulary, term=\"internet\", top_n=5)\n",
    "    ['devices', 'privacy', 'era', 'allow', 'rules']\n",
    "    >>> find_similar_terms(term_similarity_matrix, vocabulary, term=\"college\", top_n=3)\n",
    "    ['football', 'admissions', 'savings']\n",
    "    \"\"\"\n",
    "    similar_tokens = []\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "\n",
    "    return similar_tokens[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8632a55-f49a-43ca-8ef0-77964a69332c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f73fa91c29218cb60f7fd056e6bc532",
     "grade": true,
     "grade_id": "cell-bde1ade7e2b76bf1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test(find_similar_terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d373664107c403e60fb0afc0f24af9e1a1d16c67a3863ead0624919cc69aa24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
